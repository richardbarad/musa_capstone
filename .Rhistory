clusters AS cluster_num,
CASE clusters
WHEN 1 THEN 'A'
WHEN 2 THEN 'B'
WHEN 3 THEN 'C'
WHEN 4 THEN 'D'
WHEN 5 THEN 'E'
WHEN 6 THEN 'F'
WHEN 7 THEN 'G'
WHEN 8 THEN 'H'
WHEN 9 THEN 'I'
ELSE NULL
END AS cluster_letter,
--CAST(clusters AS STRING) AS assigned_cluster,
ROUND(flood_pct,2) AS flooding,
ROUND(pct_minority,2) AS minority_pop,
ROUND(pct_non_english,2) AS non_english_pop,
ROUND(pct_poverty,2) AS poverty,
ROUND(pct_no_university,2) AS no_uni,
ROUND(health_hazard_index,2) AS health_hazard_index,
ROUND(air_index,2) AS air_index,
ROUND(fire,2) AS fire,
ROUND(waste_proximity,2) AS water_proximity,
ROUND(Wastewater,2) AS wastewater,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY flood_pct) * 100, 0) AS int64) AS flood_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY pct_minority) * 100, 0) AS int64) AS minority_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY pct_non_english) * 100, 0) AS int64) AS non_englih_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY pct_poverty) * 100, 0) AS int64) AS poverty_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY pct_no_university) * 100, 0) AS int64) AS no_uni_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY air_index) * 100, 0) AS int64) AS air_index_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY health_hazard_index) * 100, 0) AS int64) AS health_hazard_index_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY fire) * 100, 0) AS int64) AS fire_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY waste_proximity) * 100, 0) AS int64) AS waste_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY Wastewater) * 100, 0) AS int64) AS wastewater_percentile_rank,
CASE
WHEN NTILE(4) OVER (ORDER BY flood_pct) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY flood_pct) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY flood_pct) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY flood_pct) = 4 THEN 'Highest 25%'
END AS flood_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY pct_minority) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY pct_minority) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_minority) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_minority) = 4 THEN 'Highest 25%'
END AS minority_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY pct_non_english) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY pct_non_english) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_non_english) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_non_english) = 4 THEN 'Highest 25%'
END AS non_english_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY pct_poverty) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY pct_poverty) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_poverty) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_poverty) = 4 THEN 'Highest 25%'
END AS poverty_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY pct_no_university) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY pct_no_university) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_no_university) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_no_university) = 4 THEN 'Highest 25%'
END AS no_uni_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY air_index) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY air_index) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY air_index) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY air_index) = 4 THEN 'Highest 25%'
END AS air_index_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY health_hazard_index) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY health_hazard_index) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY health_hazard_index) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY health_hazard_index) = 4 THEN 'Highest 25%'
END AS health_hazard_index_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY fire) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY fire) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY fire) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY fire) = 4 THEN 'Highest 25%'
END AS fire_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY waste_proximity) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY waste_proximity) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY waste_proximity) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY waste_proximity) = 4 THEN 'Highest 25%'
END AS waste_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY Wastewater) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY Wastewater) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY Wastewater) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY Wastewater) = 4 THEN 'Highest 25%'
END AS wastewater_quartile,
st_geogfromtext(geometry, make_valid => TRUE) AS geography
FROM results_data.results;
-- Drop the original table
DROP TABLE results_data.results_geom;
-- Rename the new table to the original name
ALTER TABLE results_data.results_geom_temp RENAME TO results_geom;
"
# Authenticate with BigQuery
bq_auth(path='C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json')
# Run the SQL query to create the new table
job <- dbSendQuery(project = project_id, query = sql_query)
bq_auth(path='C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json')
# Run the SQL query to create the new table
job <- dbSendQuery(conn = bigrquery::bigquery(), statement = sql_query)
bq_auth(path='C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json')
# Define your SQL query
sql_query <- "CREATE OR REPLACE TABLE results_data.results_geom_temp
CLUSTER BY (geography)
AS
SELECT
GEOID,
NAME AS name,
clusters AS cluster_num,
CASE clusters
WHEN 1 THEN 'A'
WHEN 2 THEN 'B'
WHEN 3 THEN 'C'
WHEN 4 THEN 'D'
WHEN 5 THEN 'E'
WHEN 6 THEN 'F'
WHEN 7 THEN 'G'
WHEN 8 THEN 'H'
WHEN 9 THEN 'I'
ELSE NULL
END AS cluster_letter,
--CAST(clusters AS STRING) AS assigned_cluster,
ROUND(flood_pct,2) AS flooding,
ROUND(pct_minority,2) AS minority_pop,
ROUND(pct_non_english,2) AS non_english_pop,
ROUND(pct_poverty,2) AS poverty,
ROUND(pct_no_university,2) AS no_uni,
ROUND(health_hazard_index,2) AS health_hazard_index,
ROUND(air_index,2) AS air_index,
ROUND(fire,2) AS fire,
ROUND(waste_proximity,2) AS water_proximity,
ROUND(Wastewater,2) AS wastewater,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY flood_pct) * 100, 0) AS int64) AS flood_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY pct_minority) * 100, 0) AS int64) AS minority_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY pct_non_english) * 100, 0) AS int64) AS non_englih_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY pct_poverty) * 100, 0) AS int64) AS poverty_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY pct_no_university) * 100, 0) AS int64) AS no_uni_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY air_index) * 100, 0) AS int64) AS air_index_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY health_hazard_index) * 100, 0) AS int64) AS health_hazard_index_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY fire) * 100, 0) AS int64) AS fire_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY waste_proximity) * 100, 0) AS int64) AS waste_percentile_rank,
CAST(ROUND(PERCENT_RANK() OVER (ORDER BY Wastewater) * 100, 0) AS int64) AS wastewater_percentile_rank,
CASE
WHEN NTILE(4) OVER (ORDER BY flood_pct) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY flood_pct) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY flood_pct) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY flood_pct) = 4 THEN 'Highest 25%'
END AS flood_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY pct_minority) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY pct_minority) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_minority) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_minority) = 4 THEN 'Highest 25%'
END AS minority_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY pct_non_english) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY pct_non_english) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_non_english) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_non_english) = 4 THEN 'Highest 25%'
END AS non_english_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY pct_poverty) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY pct_poverty) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_poverty) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_poverty) = 4 THEN 'Highest 25%'
END AS poverty_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY pct_no_university) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY pct_no_university) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_no_university) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_no_university) = 4 THEN 'Highest 25%'
END AS no_uni_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY air_index) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY air_index) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY air_index) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY air_index) = 4 THEN 'Highest 25%'
END AS air_index_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY health_hazard_index) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY health_hazard_index) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY health_hazard_index) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY health_hazard_index) = 4 THEN 'Highest 25%'
END AS health_hazard_index_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY fire) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY fire) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY fire) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY fire) = 4 THEN 'Highest 25%'
END AS fire_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY waste_proximity) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY waste_proximity) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY waste_proximity) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY waste_proximity) = 4 THEN 'Highest 25%'
END AS waste_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY Wastewater) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY Wastewater) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY Wastewater) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY Wastewater) = 4 THEN 'Highest 25%'
END AS wastewater_quartile,
st_geogfromtext(geometry, make_valid => TRUE) AS geography
FROM results_data.results;
-- Drop the original table
DROP TABLE results_data.results_geom;
-- Rename the new table to the original name
ALTER TABLE results_data.results_geom_temp RENAME TO results_geom;
"
# Construct the connection object
con <- dbConnect(
bigrquery::bigquery(),
project = "capstone-rb",
dataset = "results_data"
)
# Send the query to BigQuery and get the result
result <- dbSendQuery(con, sql_query)
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE)
library(tidyverse)
library(tigris)
library(sf)
library(httr2)
library(tidycensus)
library(sfdep)
library(RSocrata)
library(terra)
library(gridExtra)
library(corrr)
library(corrplot)
library(caret)
library(flexclust)
library(kableExtra)
library(viridis)
library(RColorBrewer)
library('ggthemes')
library(googleCloudStorageR)
library(bigrquery)
options(tigris_use_cache = TRUE)
options(scipen = 999)
sql_file = 'sql/create_table.sql'
sql_query <- read_file(sql_file)
# Authenticate with Google Cloud Storage
gcs_auth(json_file="C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json")
#Set Global BUcket
gcs_global_bucket("capstone-rb-data")
# Upload file to Cloud Storage
gcs_upload(file="results_for_biquery.csv",predefinedAcl = "bucketLevel")
#Authenticate with BigQuery
bq_auth(path='C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json')
# Define your SQL query
sql_file = 'sql/create_table.sql'
sql_query <- read_file(sql_file)
# Construct the connection object
con <- dbConnect(
bigrquery::bigquery(),
project = "capstone-rb",
dataset = "results_data"
)
# Send the query to BigQuery and get the result
result <- dbSendQuery(con, sql_query)
# Authenticate with Google Cloud Storage
gcs_auth(json_file="C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json")
#Set Global BUcket
gcs_global_bucket("capstone-rb-data")
# Upload file to Cloud Storage
gcs_upload(file="results_for_biquery.csv",predefinedAcl = "bucketLevel")
#Authenticate with BigQuery
bq_auth(path='C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json')
# Define your SQL query
sql_file = 'sql/create_table.sql'
sql_query <- read_file(sql_file)
# Construct the connection object
con <- dbConnect(
bigrquery::bigquery(),
project = "capstone-rb",
dataset = "results_data"
)
# Send the query to BigQuery and get the result
result <- dbSendQuery(con, sql_query)
# Authenticate with Google Cloud Storage
gcs_auth(json_file="C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json")
#Set Global BUcket
gcs_global_bucket("capstone-rb-data")
# Upload file to Cloud Storage
gcs_upload(file="results_for_biquery.csv",predefinedAcl = "bucketLevel")
#Authenticate with BigQuery
bq_auth(path='C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json')
# Define your SQL query
sql_file = 'sql/create_table.sql'
sql_query <- read_file(sql_file)
# Construct the connection object
con <- dbConnect(
bigrquery::bigquery(),
project = "capstone-rb",
dataset = "results_data"
)
# Send the query to BigQuery and get the result
result <- dbSendQuery(con, sql_query)
bq_auth(path='C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json')
# Define your SQL query
sql_file = 'sql/create_table.sql'
sql_query <- "CREATE OR REPLACE TABLE results_data.results_geom_temp
CLUSTER BY (geography)
AS
SELECT
GEOID,
NAME AS name,
clusters AS cluster_num,
CASE clusters
WHEN 1 THEN 'A'
WHEN 2 THEN 'B'
WHEN 3 THEN 'C'
WHEN 4 THEN 'D'
WHEN 5 THEN 'E'
WHEN 6 THEN 'F'
WHEN 7 THEN 'G'
WHEN 8 THEN 'H'
WHEN 9 THEN 'I'
ELSE NULL
END AS cluster_letter,
--CAST(clusters AS STRING) AS assigned_cluster,
ROUND(flood_pct,2) AS flooding,
ROUND(pct_minority,2) AS minority_pop,
ROUND(pct_non_english,2) AS non_english_pop,
ROUND(pct_poverty,2) AS poverty,
ROUND(pct_no_university,2) AS no_uni,
ROUND(health_hazard_index,2) AS health_hazard_index,
ROUND(air_index,2) AS air_index,
ROUND(fire,2) AS fire,
ROUND(waste_proximity,2) AS water_proximity,
ROUND(Wastewater,2) AS wastewater,
CASE
WHEN NTILE(4) OVER (ORDER BY flood_pct) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY flood_pct) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY flood_pct) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY flood_pct) = 4 THEN 'Highest 25%'
END AS flood_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY pct_minority) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY pct_minority) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_minority) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_minority) = 4 THEN 'Highest 25%'
END AS minority_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY pct_non_english) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY pct_non_english) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_non_english) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_non_english) = 4 THEN 'Highest 25%'
END AS non_english_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY pct_poverty) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY pct_poverty) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_poverty) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_poverty) = 4 THEN 'Highest 25%'
END AS poverty_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY pct_no_university) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY pct_no_university) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_no_university) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY pct_no_university) = 4 THEN 'Highest 25%'
END AS no_uni_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY air_index) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY air_index) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY air_index) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY air_index) = 4 THEN 'Highest 25%'
END AS air_index_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY health_hazard_index) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY health_hazard_index) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY health_hazard_index) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY health_hazard_index) = 4 THEN 'Highest 25%'
END AS health_hazard_index_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY fire) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY fire) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY fire) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY fire) = 4 THEN 'Highest 25%'
END AS fire_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY waste_proximity) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY waste_proximity) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY waste_proximity) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY waste_proximity) = 4 THEN 'Highest 25%'
END AS waste_quartile,
CASE
WHEN NTILE(4) OVER (ORDER BY Wastewater) = 1 THEN 'Lowest 25%'
WHEN NTILE(4) OVER (ORDER BY Wastewater) = 2 THEN '25 – 50th percentile'
WHEN NTILE(4) OVER (ORDER BY Wastewater) = 3 THEN '50th - 75th percentile'
WHEN NTILE(4) OVER (ORDER BY Wastewater) = 4 THEN 'Highest 25%'
END AS wastewater_quartile,
st_geogfromtext(geometry, make_valid => TRUE) AS geography
FROM results_data.results;
-- Drop the original table
DROP TABLE results_data.results_geom;
-- Rename the new table to the original name
ALTER TABLE results_data.results_geom_temp RENAME TO results_geom;
"
# Construct the connection object
con <- dbConnect(
bigrquery::bigquery(),
project = "capstone-rb",
dataset = "results_data"
)
# Send the query to BigQuery and get the result
result <- dbSendQuery(con, sql_query)
# Authenticate with Google Cloud Storage
gcs_auth(json_file="C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json")
#Set Global BUcket
gcs_global_bucket("capstone-rb-data")
# Upload file to Cloud Storage
gcs_upload(file="results_for_biquery.csv",predefinedAcl = "bucketLevel")
#Authenticate with BigQuery
bq_auth(path='C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json')
# Define your SQL query
sql_file = 'sql/create_table.sql'
sql_query <- read_file(sql_file)
# Construct the connection object
con <- dbConnect(
bigrquery::bigquery(),
project = "capstone-rb",
dataset = "results_data"
)
# Send the query to BigQuery and get the result
result <- dbSendQuery(con, sql_query)
# Fetch the result
data <- dbFetch(result)
# Close the connection
dbDisconnect(con)
View(data)
#This creates a csv file for biquery
results_4326 <- results %>% st_transform('EPSG:4326') %>%
mutate(geometry = st_as_text(geometry)) %>%
cbind(basin_tracts_4 %>% st_drop_geometry() %>% select(GEOID,NAME),.) %>%
st_drop_geometry()
write.csv(results_4326,'results_for_biquery.csv',row.names=FALSE)
# This step deals with updating GCS
# Authenticate with Google Cloud Storage using json credentials
gcs_auth(json_file="C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json")
gcs_global_bucket("capstone-rb-data")
# Upload file to Cloud Storage
gcs_upload(file="results_for_biquery.csv",predefinedAcl = "bucketLevel")
#Authenticate with BigQuery using json credentials
bq_auth(path='C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json')
# Define SQL query
sql_file = 'sql/create_table.sql'
sql_query <- read_file(sql_file)
# Construct the connection to bigquery
con <- dbConnect(
bigrquery::bigquery(),
project = "capstone-rb",
dataset = "results_data"
)
# Send the query to BigQuery
dbSendQuery(con, sql_query)
# Close the connection
dbDisconnect(con)
#Authenticate with BigQuery using json credentials
bq_auth(path='C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json')
bq_table_upload("results_for_biquery.csv", project = "capstone-rb", dataset = "results_data", table = "results", write_disposition = "WRITE_TRUNCATE")
bq_table_upload("capstone-rb.results_data.results","results_for_biquery.csv", write_disposition = "WRITE_TRUNCATE")
bq_table_upload("capstone-rb.results_data.results", "results_for_biquery.csv", create_disposition = "CREATE_IF_NEEDED", write_disposition = "WRITE_TRUNCATE")
bq_table_upload("capstone-rb.results_data.results", values=results_4326, create_disposition = "CREATE_IF_NEEDED", write_disposition = "WRITE_TRUNCATE")
#This creates a csv file for biquery
results_4326 <- results %>% st_transform('EPSG:4326') %>%
mutate(geometry = st_as_text(geometry)) %>%
cbind(basin_tracts_4 %>% st_drop_geometry() %>% select(GEOID,NAME),.) %>%
st_drop_geometry()
bq_table_upload("capstone-rb.results_data.results", values=results_4326, create_disposition = "CREATE_IF_NEEDED", write_disposition = "WRITE_TRUNCATE")
# Define SQL query
sql_file = 'sql/create_table.sql'
sql_query <- read_file(sql_file)
# Construct the connection to bigquery
con <- dbConnect(
bigrquery::bigquery(),
project = "capstone-rb",
dataset = "results_data"
)
# Send the query to BigQuery
dbSendQuery(con, sql_query)
# Close the connection
dbDisconnect(con)
#This creates a csv file for biquery
results_4326 <- results %>% st_transform('EPSG:4326') %>%
mutate(geometry = st_as_text(geometry)) %>%
cbind(basin_tracts_4 %>% st_drop_geometry() %>% select(GEOID,NAME),.) %>%
st_drop_geometry()
#Authenticate with BigQuery using json credentials
bq_auth(path='C:/Users/richa/GitHub/musa_capstone/keys/capstone-rb-b0c15ce34dbe.json')
#Upload Contents of results_4326 df to big_query table, overwrite existing table
bq_table_upload("capstone-rb.results_data.results", values=results_4326, write_disposition = "WRITE_TRUNCATE")
# Define SQL query
sql_file = 'sql/create_table.sql'
sql_query <- read_file(sql_file)
# Construct the connection to bigquery
con <- dbConnect(
bigrquery::bigquery(),
project = "capstone-rb",
dataset = "results_data"
)
# Send the query to BigQuery
dbSendQuery(con, sql_query)
# Close the connection
dbDisconnect(con)
